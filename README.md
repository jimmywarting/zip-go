# ZIP-go <img src="https://user-images.githubusercontent.com/1148376/183421896-8fea5bef-6d32-4f49-ab6c-f2fe7e6ac4ab.svg" width="20px" height="20px" title="This package contains built-in JSDoc declarations (...works as equally well as d.ts)" alt="JSDoc icon, indicating that this package has built-in type declarations">

`zip-go` was designed with the goal of saving multiple large files generated by
the browsers without holding any of the data in memory by streaming/piping data
to a destination. While the zip format isn't technically built for streaming and
each file entry needs some pre-header information that tells how large a file is
and what the crc checksum is... all of this can be stored in the central
directory header also at the end of a zip file...

## Install

`zip-go` is an ESM-only module - you are not able to import it with `require`. If you are unable to use ESM in your project you can use the async `import('zip-go')` from CommonJS to load `zip-go` asynchronously.<br>
`npm install zip-go`

## Requirements

- `BigInt` support
- `ReadableStream`
- `WritableStream`
- Reading compressed file entries requires [DecompressionStream](https://developer.mozilla.org/en-US/docs/Web/API/DecompressionStream#browser_compatibility)
- Reading a zip file entries is done with `Blob`-like objects. NodeJS users can
use [fs.openAsBlob](https://nodejs.org/dist/latest/docs/api/fs.html#fsopenasblobpath-options) or [fetch-blob](https://github.com/node-fetch/fetch-blob/)

Supports both reading and writing ZIP64 files larger than 4 GiB.

## Creating a ZIP

```js
import Writer from 'zip-go/lib/write.js'

const s3 = 'https://s3-us-west-2.amazonaws.com/bencmbrook/'
const files = ['NYT.txt', 'water.png', 'Earth.jpg'].values()

// Creates a regular ReadableStream that will pull file like objects
const myReadable = new ReadableStream({
  async pull(controller) {
    const { done, value } = files.next()
    if (done) return controller.close()
    const { body } = await fetch(s3 + value)

    return controller.enqueue({
      name: `/${value}`,
      stream: () => body,
    })
  }
})

const stream = myReadable
  .pipeThrough(new ZipWriter())

const blob = await new Response(stream).blob()
```

if you would like to work it more manually you can do that as well.

```js
import StreamSaver from 'streamsaver';
import Writer from 'zip-go/lib/write.js';

const { readable, writable } = new Writer();
const writer = writable.getWriter();

// Set up StreamSaver
const fileStream = streamSaver.createWriteStream('archive.zip');

// Add a WebIDL File like object that at least have name and a stream method
// that returns a whatwg ReadableStream
writer.write({
  name: '/cat.txt',
  lastModified: +new Date(123),
  stream: () => new Response('mjau').body
})

writer.write(
  new File(['woof'], 'dog.txt', { lastModified: +new Date(123)})
)

readable.pipeTo(destination)

writer.close()
```

## Memory-efficient ZIP creation with filesystem

For very large files (especially ZIP64 files > 4GB), loading the entire ZIP into memory can be problematic. You can use the filesystem as intermediate storage:

```js
import fs from 'node:fs/promises'
import { openAsBlob } from 'node:fs'
import Writer from 'zip-go/lib/write.js'

// Write ZIP to filesystem first, then open as Blob
async function createZipBlobFromFS(files, destPath) {
  // Write the ZIP stream directly to a file
  const stream = ReadableStream.from(files).pipeThrough(new Writer())
  await fs.writeFile(destPath, stream)
  
  // Open the file as a Blob (memory-efficient)
  const blob = await openAsBlob(destPath)
  
  return blob
}

// Usage
const largeFiles = [
  new VirtualFile(5 * 1024 * 1024 * 1024, 'large-file.bin') // 5GB file
]

const zipBlob = await createZipBlobFromFS(largeFiles, '/tmp/large-archive.zip')

// The blob can now be used without loading the entire ZIP into memory
// For example, you can read it with the zip reader or serve it via HTTP
```

This approach is especially useful for:
- Creating ZIP64 archives larger than 4GB
- Server-side ZIP creation where memory is limited
- Processing many large files in sequence

## Reading a zip

This read method only read the central directory (end of the file)
to figure out all about each entry. Each `Entry` returns a WebIDL `File` like
object with added properties that are more zip specific

```js
import read from 'zip-go/lib/read.js'

for await (const entry of read(blob)) {
  console.log(entry)
  console.log(entry.name)
  console.log(entry.size)
  console.log(entry.type)
  console.log(entry.directory)

  const ab = await entry.arrayBuffer()
  const text = await entry.text()
  const readableStream = entry.stream()

  // returns a real web File Object, if the entry is uncompressed
  // it will just slice the zip with it's start/end position
  const file = await entry.file()
}
```

## Using Entry class for generating ZIP files

The `Entry` class can now be used in write mode to generate ZIP file headers and descriptors. This is useful for advanced ZIP generation scenarios or implementing custom ZIP writers.

```js
import { Entry } from 'zip-go/lib/read.js'
import Crc32 from 'zip-go/lib/crc.js'
import fs from 'node:fs'

// Create an Entry for writing
const blob = await fs.openAsBlob('path/to/file.txt')
const entry = new Entry(blob)

// Set entry properties
entry.name = 'readme.md'
entry.compressionMethod = 0 // No compression
entry.size = blob.size
entry.compressedSize = blob.size

// Calculate and set CRC32
const crc = new Crc32()
const content = await blob.arrayBuffer()
crc.append(new Uint8Array(content))
entry.crc32 = crc.get()

// Generate ZIP structures
const localHeader = entry.generateLocalHeader() // 30+ byte header
const dataDescriptor = entry.generateDataDescriptor() // 16 byte descriptor

// For streaming scenarios where CRC is unknown at header write time:
const entry2 = new Entry(blob)
entry2.name = 'stream.txt'
// Don't set crc32 - generateLocalHeader() will set the data descriptor flag
const header = entry2.generateLocalHeader()
// Write file data...
// Calculate CRC and sizes...
entry2.crc32 = calculatedCrc
entry2.compressedSize = actualCompressedSize
entry2.size = actualUncompressedSize
const descriptor = entry2.generateDataDescriptor()
```
